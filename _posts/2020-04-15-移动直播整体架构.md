---
layout: mypost
title: 移动直播整体架构
categories: [直播, 架构]
---

一.移动直播的基本架构图如下所示：

![livevideo-arch](livevideo-arch.png)

移动直播整体架构大致可分为五个部分：

1.主播端。主要负责音视频数据的采集、预览、处理(美声、美颜、滤镜等)、编码及将编码后的数据推送至源站（可能经过上行加速节点）。

2.源站。该部分属于云服务的一项功能，接收来自主播端的音视频数据，当来自CDN网络（下行节点）的数据拉取请求时，按照对应的格式返回给CDN。同时也担负将直播音视频数据落盘，生成点播回看视频。

3.转码。可以从源站拉取一路流，转码成多种分辨率、码率，再回推给源站。这样实现了一路主播视频流的推送，制造出码率不同的多路流。拉取器是转码的一种变种，它从其他源处拉取数据流（使用某种约定好的拉流协议），并remux成rtmp推送给源站。

4.CDN。上下行节点都归为数据分发网络。该部分属于云服务的一项功能，多层下行节点从源站获取直播音视频数据，然后将数据分发给各地观众。

5.观众端。从下行节点获取直播数据，解析、解码并渲染音视频，以供观众观看。

如果一帧画面在主播侧被采集时刻为t0，某观众屏幕上展示出这帧画面的时候为t1，那么该观众能感知的延时为t1-t0。这个延时，我们叫直播延时。主播端从摄像头、麦克风采集音视频数据，在移动端处理编码后，经由源站、CDN直至观众端解码并渲染播放整个链路引入的延时，直播延时涵盖了整个链路的完整延时。


二. 直播链路各模块对延时的"贡献"

直播延时大致可分为两个部分：

音视频数据在直播网络链路传输所引入的延时，此部分无法避免；
直播链路各模块对音视频数据的cache、process操作引入的延时，则可以采用一定方法降低甚至消除；
下面将分析各模块对于直播延时的"贡献"：


2.1 主播端

主播端在采集音视频数据后基本流程如下所示：

![flow_path](flow_path.png)


1.采集。首先使用麦克风采集音频，使用摄像头采集画面。在此时，打上对应的时间戳t0。
处理。音频可以加上混响。画面可以做各种滤镜处理。

2.混合。将背景音乐、麦克风声音混合。将摄像头画面与背景图、连麦画面等做图层叠加。
预览。预览包括两部分：

3.耳返。音频处理后的数据，即可送入耳返通道，此时主播能听到t0时刻的声音。主播耳朵提到对应声音时刻为t1。t1-t0表征了主播唱出一个词，到耳朵里面听到这个词的耗时。主播对耳返延时的要求比较高，该部分延时较小，大约40ms至80ms。

4.画面预览。 图像混合后的数据，接入主播屏幕画面预览，此时主播屏幕上的画面渲染时刻为t1。一般来说t1与t0延时极其小。如果t1-t0大于40ms，人眼即能有所感知delay。
如果Android设备不支持Low-Latency时，耳返功能本身耗时较大，大约300ms以上，但是并不影响直播整体延时。

5.编码。如果直播准备采用30fps推流，那么视频编码需要达到至少30fps的性能。每帧编码耗时需要控制在33ms以下。整个编码的耗时除了单帧耗时，还有B帧参数数量。编码器配置和编码性能会引入耗时。
网络自适应内部有个发送buffer，用于监控网络发送情况，并在网络恶劣情况下丢掉待发送的码流数据。基本逻辑如下(可以看到，只在网络从良好到恶劣的转变过程中，临时引入延时)：
网络良好时，发送buffer内为空。该环节不引入延时。
网络恶劣时，发送buffer堆积，超过阈值触发丢帧。该环节引入固定延时。
网络恶劣时，监控buffer堆积，反馈编码器降低输出码率，buffer堆积情况转好，直至清空buffer。清空后延时归零。

6.封包。flv封包过程简单，不引入延时。

7.发送。对于不同的协议：
RTMP推流层不引入延时，客户端tcp协议栈buffer延时很小，可以忽略。TCP引入的延时主要在高丢包、高重传率网络下，链路引入的延时。
基于UDP的私有推流协议，协议层可能引入buffer，依照实际情况而定。高丢包或者高重传率的网络情况下，链路延时UDP优于TCP。

总结就是，推流端经过不懈努力，除了突变的网络情况临时引入的buffer延时，推流SDK的延时主要是滤镜处理(gpu性能相关)、编码性能引入的延时(cpu性能相关)。该延时一般在100ms左右。

2.2 上行节点
上行节点会透明转发数据，合理的上行加速，会降低主播直连源站的链路延时。

同时上行节点也支持就近分发，也能降低链路延时。

2.3 源站
源站在接收直播数据时会缓存该路直播的最新音视频数据，一般为若干个GOP，某CDN节点初次向源站请求某直播流数据时，源站会将缓存的数据全部传给该CDN节点。
在CDN已与源站建立链接并拉取该路直播的数据时，源站会将最新的数据转发给CDN。

2.4 转码
转码服务从源站拉取直播流，并转码转推回源站。此时会引入转码延时。实时转码延时一般会引入100ms-200ms延时。

2.5 拉取器
从其他数据源拉取直播流后，转推到源站。
如果拉取器与数据源带宽满足实时传输的前提下，延时主要依赖数据源的延时。

2.6 下行节点
在第一次接收到播放某直播流的请求后，CDN边缘节点会通过CDN网络拉取该直播流的数据并缓存最新若干*gop的数据，以便应答后续可能的播放请求。
当某一个观众端发起播放请求，播放器在与CDN节点初次建立链接后，播放器会快速从CDN边缘节点读取其缓存数据直至读取到最新数据。在播放器耗尽对应的gop缓存前，下行节点引入了短暂的延时。
耗尽gop缓存的场景大致几种：

1.播放端拉流速度足够快，会很快耗尽该buffer；
2.播放端拉流速度和直播流码率相差不大，该buffer长期位于CDN边缘节点，该部分缓存无法清除；
3.播放器拉流速度低于直播流码率，播放端频繁卡顿，该buffer持续增长，触发CDN边缘节点对buffer的丢帧逻辑。该场景的延时等于CDN边缘节点的buffer最大阈值。该情况下，观众端观看体验很差，应该通过客户端监控断开连接并选择更低码率的直播流。

一般情况下，用户场景主要在1场景下，即观众拉流速度最大值大于直播流码率。下文重点考虑该场景。

2.7 播放端
观众端开始播放某直播流，大量gop cache数据到了播放器内存，这部分缓存是影响直播延时的关键部分。

举个例子，该直播流gop为3秒，CDN边缘节点gop配置为6秒。观众端拉流速度足够快，开播后，播放器内会出现6至9秒的音视频数据。

本文的核心考量是如何快速消耗这部分数据，以达到降低直播延时的目的。




